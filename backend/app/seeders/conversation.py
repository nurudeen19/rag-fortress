"""Conversation and Message seeder for chat history simulation."""

import logging
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.future import select
from datetime import datetime, timezone, timedelta
import uuid

from app.models.conversation import Conversation
from app.models.message import Message, MessageRole
from app.models.user import User
from app.seeders.base import BaseSeed

logger = logging.getLogger(__name__)


class ConversationSeeder(BaseSeed):
    """Seeder for creating sample conversations and messages."""
    
    name = "conversations"
    description = "Creates sample conversations and messages for testing chat history"
    required_tables = ["users", "conversations", "messages"]
    
    # Sample conversation templates
    CONVERSATIONS = [
        {
            "title": "Machine Learning Project Implementation",
            "messages": [
                {"role": MessageRole.USER, "content": "How do I implement a neural network in Python?"},
                {"role": MessageRole.ASSISTANT, "content": "You can use TensorFlow or PyTorch for neural networks. TensorFlow is more beginner-friendly with Keras API. Here's a basic example:\n\n```python\nimport tensorflow as tf\nfrom tensorflow import keras\n\nmodel = keras.Sequential([\n    keras.layers.Dense(128, activation='relu', input_shape=(28*28,)),\n    keras.layers.Dropout(0.2),\n    keras.layers.Dense(10, activation='softmax')\n])\n\nmodel.compile(optimizer='adam', loss='sparse_categorical_crossentropy', metrics=['accuracy'])\nmodel.fit(x_train, y_train, epochs=10)\n```\n\nFor more complex architectures, consider using PyTorch for greater flexibility.", "token_count": 234, "meta": {"sources": [{"document": "tensorflow_docs.pdf", "score": 0.98}]}},
                {"role": MessageRole.USER, "content": "What about data preprocessing for training?"},
                {"role": MessageRole.ASSISTANT, "content": "Data preprocessing is crucial for model performance. Key steps include:\n\n1. **Normalization**: Scale features to 0-1 range using MinMaxScaler\n2. **Standardization**: Zero mean and unit variance using StandardScaler\n3. **Handling Missing Values**: Use mean/median imputation or forward fill\n4. **Outlier Detection**: Remove or cap extreme values\n5. **Data Splitting**: 70% train, 15% validation, 15% test\n\nExample:\n```python\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.model_selection import train_test_split\n\nscaler = StandardScaler()\nX_scaled = scaler.fit_transform(X)\nX_train, X_test, y_train, y_test = train_test_split(X_scaled, y, test_size=0.3)\n```", "token_count": 312, "meta": {"sources": [{"document": "ml_best_practices.pdf", "score": 0.95}]}},
                {"role": MessageRole.USER, "content": "How do I evaluate model performance?"},
                {"role": MessageRole.ASSISTANT, "content": "Common evaluation metrics depend on your task:\n\n**Classification:**\n- Accuracy: (TP+TN)/(TP+TN+FP+FN)\n- Precision: TP/(TP+FP) - Positive prediction accuracy\n- Recall: TP/(TP+FN) - True positive detection rate\n- F1-Score: Harmonic mean of precision and recall\n- ROC-AUC: Measures performance across thresholds\n\n**Regression:**\n- MSE (Mean Squared Error): Average of squared residuals\n- RMSE: Square root of MSE\n- MAE (Mean Absolute Error): Average absolute residuals\n- R² Score: Proportion of variance explained\n\nExample:\n```python\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n\ny_pred = model.predict(X_test)\nacc = accuracy_score(y_test, y_pred)\nprecision = precision_score(y_test, y_pred)\nrecall = recall_score(y_test, y_pred)\nf1 = f1_score(y_test, y_pred)\n```", "token_count": 425, "meta": {"sources": [{"document": "ml_metrics_guide.pdf", "score": 0.97}]}},
                {"role": MessageRole.USER, "content": "Any tips for avoiding overfitting?"},
                {"role": MessageRole.ASSISTANT, "content": "Overfitting occurs when a model learns training data too well, including noise. Prevention strategies:\n\n1. **Use More Data**: Collect additional training samples\n2. **Feature Selection**: Remove irrelevant features\n3. **Regularization**: L1/L2 penalties on weights\n4. **Dropout**: Randomly disable neurons during training\n5. **Early Stopping**: Stop training when validation loss increases\n6. **Cross-Validation**: Use k-fold validation for robust estimates\n7. **Ensemble Methods**: Combine multiple models\n8. **Hyperparameter Tuning**: Use validation set to tune parameters\n\nFor deep learning:\n```python\nmodel.add(keras.layers.Dropout(0.3))  # 30% dropout\nearly_stop = keras.callbacks.EarlyStopping(monitor='val_loss', patience=3)\nmodel.fit(X_train, y_train, epochs=100, callbacks=[early_stop], validation_split=0.2)\n```", "token_count": 298, "meta": {"sources": [{"document": "overfitting_prevention.pdf", "score": 0.96}]}},
                {"role": MessageRole.USER, "content": "Can you explain cross-validation in more detail?"},
                {"role": MessageRole.ASSISTANT, "content": "Cross-validation is a resampling technique to evaluate model performance robustly:\n\n**K-Fold Cross-Validation:**\n1. Split data into K equal-sized folds\n2. For each fold: train on K-1 folds, test on remaining fold\n3. Average the K test scores\n\nBenefits:\n- Uses all data for both training and testing\n- Reduces variance in performance estimates\n- Better for small datasets\n\n```python\nfrom sklearn.model_selection import cross_val_score, KFold\n\nkf = KFold(n_splits=5, shuffle=True)\nscores = cross_val_score(model, X, y, cv=kf, scoring='accuracy')\nprint(f'Mean: {scores.mean():.3f}, Std: {scores.std():.3f}')\n```\n\n**Other Variants:**\n- **Stratified K-Fold**: Maintains class distribution in each fold\n- **Leave-One-Out (LOO)**: Use single sample for test set\n- **Time Series Split**: Respects temporal order for time-series data", "token_count": 287, "meta": {"sources": [{"document": "cross_validation_guide.pdf", "score": 0.94}]}},
                {"role": MessageRole.USER, "content": "What about hyperparameter tuning?"},
                {"role": MessageRole.ASSISTANT, "content": "Hyperparameter tuning is critical for model performance. Methods:\n\n**1. Grid Search**: Try all combinations\n```python\nfrom sklearn.model_selection import GridSearchCV\n\nparam_grid = {\n    'learning_rate': [0.001, 0.01, 0.1],\n    'max_depth': [3, 5, 7, 10],\n    'n_estimators': [50, 100, 200]\n}\n\ngrid = GridSearchCV(estimator, param_grid, cv=5)\ngrid.fit(X_train, y_train)\nprint(f'Best params: {grid.best_params_}')\n```\n\n**2. Random Search**: Sample random combinations\n```python\nfrom sklearn.model_selection import RandomizedSearchCV\n\nrandom = RandomizedSearchCV(estimator, param_grid, n_iter=20, cv=5)\nrandom.fit(X_train, y_train)\n```\n\n**3. Bayesian Optimization**: Learn from previous trials\n```python\nfrom skopt import gp_minimize\n\ndef objective(params):\n    model = create_model(params)\n    score = cross_val_score(model, X, y).mean()\n    return -score  # Minimize negative score\n\nresult = gp_minimize(objective, space, n_calls=20)\n```", "token_count": 356, "meta": {"sources": [{"document": "hyperparameter_tuning.pdf", "score": 0.95}]}},
                {"role": MessageRole.USER, "content": "Which hyperparameters are most important for neural networks?"},
                {"role": MessageRole.ASSISTANT, "content": "Key neural network hyperparameters ranked by impact:\n\n**Critical (High Impact):**\n1. **Learning Rate**: Controls step size during gradient descent\n   - Too high: divergence, too low: slow convergence\n   - Typical range: 0.0001 to 0.1\n   - Use learning rate scheduling for better results\n\n2. **Batch Size**: Number of samples per update\n   - Small (32): Better generalization, noisier updates\n   - Large (256+): Faster training, may overfit\n   - Sweet spot: Often 32-128\n\n3. **Number of Epochs**: Training iterations\n   - Monitor validation loss for early stopping\n   - Prevents overfitting\n\n**Important (Medium Impact):**\n1. **Network Architecture**:\n   - Number of layers\n   - Neurons per layer\n   - Activation functions (ReLU, tanh, sigmoid)\n\n2. **Regularization**:\n   - Dropout rate (typically 0.2-0.5)\n   - L1/L2 penalty (weight decay)\n\n**Fine-tuning (Lower Impact):**\n1. Optimizer choice (Adam, SGD, RMSprop)\n2. Initialization method (Xavier, He)\n3. Batch normalization parameters", "token_count": 334, "meta": {"sources": [{"document": "nn_hyperparameters.pdf", "score": 0.96}]}},
                {"role": MessageRole.USER, "content": "Can you recommend a training workflow?"},
                {"role": MessageRole.ASSISTANT, "content": "Recommended ML model development workflow:\n\n**Phase 1: Data Preparation**\n- Load and explore data\n- Handle missing values\n- Remove or handle outliers\n- Feature engineering\n- Data splitting (70/15/15)\n- Normalization/Standardization\n\n**Phase 2: Baseline Model**\n- Start simple (linear regression, logistic regression)\n- Establish baseline metrics\n- Quick iteration cycle\n\n**Phase 3: Model Development**\n- Try different algorithms\n- Cross-validation evaluation\n- Focus on high-impact hyperparameters\n- Track experiments with MLflow or Weights & Biases\n\n**Phase 4: Hyperparameter Optimization**\n- Grid/Random/Bayesian search\n- Use cross-validation\n- Early stopping for neural networks\n- Threshold tuning for classification\n\n**Phase 5: Evaluation & Validation**\n- Test set evaluation (touch only once!)\n- Error analysis\n- Feature importance analysis\n- Business metrics alignment\n\n**Phase 6: Production Deployment**\n- Model serialization\n- API development\n- Monitoring and logging\n- Regular retraining pipeline", "token_count": 312, "meta": {"sources": [{"document": "ml_workflow.pdf", "score": 0.97}]}},
                {"role": MessageRole.USER, "content": "What's the difference between validation and test sets?"},
                {"role": MessageRole.ASSISTANT, "content": "Critical distinction in model evaluation:\n\n**Validation Set (15% of data):**\n- Used DURING training\n- Purpose: Tune hyperparameters\n- Evaluate different models\n- Monitor for overfitting\n- Can touch multiple times\n- Used for early stopping\n\n**Test Set (15% of data):**\n- Used AFTER training complete\n- Purpose: Final performance estimate\n- Should NEVER touch during development\n- Represents real-world performance\n- Touch only ONCE\n- Reports generalization ability\n\n**Workflow:**\n```\nRaw Data (100%)\n    ↓\n[Training Set 70%] → Train model → Monitor on Validation Set 15%\n    ↓\n[Tune hyperparameters based on validation performance]\n    ↓\n[Final evaluation on Test Set 15%]\n    ↓\n[Report test set metrics as final results]\n```\n\n**Common Mistakes:**\n- ❌ Using test set to tune hyperparameters (data leakage)\n- ❌ Touching test set multiple times (overfitting to test set)\n- ❌ Combining validation and test sets\n- ❌ Preprocessing data before splitting (leakage)\n\nRemember: Test set performance is your honest estimate of real-world performance!", "token_count": 378, "meta": {"sources": [{"document": "train_val_test_guide.pdf", "score": 0.98}]}},
                {"role": MessageRole.USER, "content": "Perfect! I'll implement this workflow for my project. Thanks!"},
                {"role": MessageRole.ASSISTANT, "content": "You're welcome! Here's your checklist:\n\n✓ Prepare and split data properly\n✓ Start with a simple baseline\n✓ Use cross-validation\n✓ Track all experiments\n✓ Tune hyperparameters systematically\n✓ Never touch the test set during development\n✓ Evaluate on test set only at the end\n✓ Document your findings\n✓ Plan for model monitoring in production\n\nGood luck with your ML project! Feel free to reach out if you have more questions.", "token_count": 156},
                {"role": MessageRole.USER, "content": "One more thing - what about imbalanced datasets?"},
                {"role": MessageRole.ASSISTANT, "content": "Handling imbalanced datasets is crucial:\n\n**Problem:**\nWhen one class is much more frequent than others, models bias toward majority class.\n\n**Detection:**\n```python\nfrom collections import Counter\nprint(Counter(y))  # Check class distribution\n```\n\n**Solutions:**\n\n**1. Resampling Techniques:**\n- **Oversampling**: Replicate minority class\n  ```python\n  from imblearn.over_sampling import RandomOverSampler, SMOTE\n  smote = SMOTE(random_state=42)\n  X_resampled, y_resampled = smote.fit_resample(X, y)\n  ```\n\n- **Undersampling**: Remove majority class samples\n  ```python\n  from imblearn.under_sampling import RandomUnderSampler\n  ```\n\n**2. Class Weights:**\n```python\nfrom sklearn.utils.class_weight import compute_class_weight\nweights = compute_class_weight('balanced', classes=np.unique(y), y=y)\nmodel.fit(X, y, sample_weight=weights)\n```\n\n**3. Appropriate Metrics:**\n- ❌ Accuracy (misleading)\n- ✓ Precision, Recall, F1-Score\n- ✓ ROC-AUC\n- ✓ Confusion Matrix\n\n**4. Ensemble Methods:**\n- Balanced Random Forest\n- EasyEnsemble\n- BalancedBagging", "token_count": 401, "meta": {"sources": [{"document": "imbalanced_datasets.pdf", "score": 0.95}]}},
                {"role": MessageRole.USER, "content": "Should I use SMOTE or simple oversampling?"},
                {"role": MessageRole.ASSISTANT, "content": "SMOTE vs Oversampling comparison:\n\n**Simple Random Oversampling:**\nPros:\n- Fast and simple\n- No additional parameters\nCons:\n- Creates exact duplicates\n- High overfitting risk\n- No new information\n\n**SMOTE (Synthetic Minority Over-sampling Technique):**\nHow it works:\n- Finds K nearest neighbors of minority samples\n- Creates synthetic samples between neighbors\n- Increases feature space coverage\n\nPros:\n- Generates synthetic data (reduces exact duplicates)\n- Better generalization\n- Recommended approach\n\nCons:\n- Slower (creates synthetic samples)\n- More parameters to tune (k_neighbors)\n- Can create outliers\n\n**Recommendation:**\n1. Start with SMOTE for most cases\n2. Combine with undersampling of majority class\n3. Validate with cross-validation\n\n```python\nfrom imblearn.pipeline import Pipeline\nfrom imblearn.combine import SMOTEENN\n\npipeline = Pipeline([\n    ('smote_enn', SMOTEENN(random_state=42)),\n    ('classifier', RandomForestClassifier())\n])\npipeline.fit(X_train, y_train)\n```\n\nAlways evaluate on original (unbalanced) test set!", "token_count": 356, "meta": {"sources": [{"document": "smote_guide.pdf", "score": 0.94}]}},
                {"role": MessageRole.USER, "content": "Great insights! I have everything I need now."},
                {"role": MessageRole.ASSISTANT, "content": "Excellent! You've covered all the important concepts:\n- Data preparation and splitting\n- Cross-validation\n- Hyperparameter tuning\n- Evaluation metrics\n- Handling imbalanced data\n\nNext steps:\n1. Apply to your dataset\n2. Document your approach\n3. Track experiments\n4. Iterate based on results\n\nGood luck with your ML implementation!", "token_count": 189}, {"role": MessageRole.USER, "content": "What are the best practices for database schema design?"},
                {"role": MessageRole.ASSISTANT, "content": "Key database design principles:\n\n1. **Normalization**: Reduce redundancy and improve data integrity\n   - 1NF: Atomic values only\n   - 2NF: Remove partial dependencies\n   - 3NF: Remove transitive dependencies\n   - BCNF: Advanced normalization\n\n2. **Entity-Relationship Model**: Design clear relationships\n   - One-to-One: Single related records\n   - One-to-Many: Single parent, multiple children\n   - Many-to-Many: Multiple parents, multiple children\n\n3. **Constraints**: Ensure data integrity\n   - PRIMARY KEY: Unique identifier\n   - FOREIGN KEY: Reference integrity\n   - UNIQUE: No duplicate values\n   - CHECK: Validate value ranges\n\n4. **Indexing**: Improve query performance\n   - Single column indexes for WHERE clauses\n   - Composite indexes for multi-column queries\n   - Avoid over-indexing (slows inserts/updates)", "token_count": 356, "meta": {"sources": [{"document": "database_design.pdf", "score": 0.98}]}},
                {"role": MessageRole.USER, "content": "How do I optimize slow SQL queries?"},
                {"role": MessageRole.ASSISTANT, "content": "Query optimization techniques:\n\n1. **Analysis Tools**:\n   - EXPLAIN PLAN: Understand query execution\n   - EXPLAIN ANALYZE: Get actual execution stats\n   - Index usage analysis\n\n2. **Common Optimizations**:\n   ```sql\n   -- Use EXPLAIN ANALYZE to identify bottlenecks\n   EXPLAIN ANALYZE SELECT * FROM users u \n   JOIN orders o ON u.id = o.user_id\n   WHERE u.created_at > '2025-01-01';\n   ```\n\n3. **Key Strategies**:\n   - Add indexes on WHERE, JOIN, and ORDER BY columns\n   - Use SELECT with specific columns, not SELECT *\n   - Use JOINS instead of subqueries when possible\n   - Limit results with appropriate WHERE clauses\n   - Use EXPLAIN to verify index usage\n   - Consider materialized views for complex queries\n   - Archive old data to reduce table size", "token_count": 289, "meta": {"sources": [{"document": "query_optimization.pdf", "score": 0.94}]}},
                {"role": MessageRole.USER, "content": "What about denormalization? When should I use it?"},
                {"role": MessageRole.ASSISTANT, "content": "Denormalization is a trade-off strategy:\n\n**When to Denormalize:**\n- High read volume, low write volume\n- Reporting/analytics queries that need aggregated data\n- Performance bottlenecks identified in normalized design\n- Complex joins affecting response time significantly\n\n**Common Denormalization Techniques:**\n\n1. **Redundant Storage**:\n   ```sql\n   -- Store user_name in orders instead of joining users table\n   CREATE TABLE orders (\n       id INT PRIMARY KEY,\n       user_id INT,\n       user_name VARCHAR(255),  -- Denormalized\n       total_amount DECIMAL(10,2),\n       created_at TIMESTAMP\n   );\n   ```\n\n2. **Aggregate Storage**:\n   ```sql\n   -- Store order_count in users table\n   CREATE TABLE users (\n       id INT PRIMARY KEY,\n       name VARCHAR(255),\n       order_count INT,  -- Updated via trigger\n       total_spent DECIMAL(10,2)  -- Updated via trigger\n   );\n   ```\n\n3. **Materialized Views**:\n   ```sql\n   -- Pre-compute complex aggregations\n   CREATE MATERIALIZED VIEW monthly_sales AS\n   SELECT DATE_TRUNC('month', created_at) as month,\n          SUM(total_amount) as total\n   FROM orders\n   GROUP BY DATE_TRUNC('month', created_at);\n   ```\n\n**Challenges:**\n- Data consistency (use triggers to update redundant data)\n- Increased storage requirements\n- More complex update logic", "token_count": 401, "meta": {"sources": [{"document": "denormalization_guide.pdf", "score": 0.92}]}},
                {"role": MessageRole.USER, "content": "Can you explain transaction isolation levels?"},
                {"role": MessageRole.ASSISTANT, "content": "Transaction isolation levels prevent concurrent access issues:\n\n**Four Isolation Levels:**\n\n1. **Read Uncommitted** (Lowest)\n   - Dirty reads possible\n   - Fastest performance\n   - Rarely used in production\n\n2. **Read Committed** (Default for most DBs)\n   - Prevents dirty reads\n   - Non-repeatable reads possible\n   - Good balance\n\n3. **Repeatable Read** (MySQL InnoDB default)\n   - Prevents dirty and non-repeatable reads\n   - Phantom reads possible\n   - Higher consistency\n\n4. **Serializable** (Highest)\n   - Prevents all anomalies\n   - Slowest performance\n   - Complete isolation\n\n**Example:**\n```sql\nSET TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nBEGIN;\nSELECT balance FROM accounts WHERE id = 1;\n-- Other operations\nCOMMIT;\n```\n\n**Trade-offs:**\n- Higher isolation = more consistency but slower\n- Lower isolation = faster but data anomalies possible", "token_count": 334, "meta": {"sources": [{"document": "transaction_isolation.pdf", "score": 0.96}]}},
                {"role": MessageRole.USER, "content": "How do I handle database migrations safely?"},
                {"role": MessageRole.ASSISTANT, "content": "Safe database migration strategies:\n\n**Best Practices:**\n\n1. **Backward Compatibility**:\n   - Add columns as nullable first\n   - Don't remove columns immediately\n   - Support both old and new column names during transition\n\n2. **Deployment Steps**:\n   ```\n   1. Deploy code that can handle both old and new schema\n   2. Run migration scripts\n   3. Verify data integrity\n   4. Deploy final code using new schema\n   5. Clean up old columns/tables after verification\n   ```\n\n3. **Testing**:\n   - Test migrations on copy of production data\n   - Verify rollback procedures\n   - Test with expected data volume\n   - Check index performance after migration\n\n4. **Tools:**\n   - Alembic (Python)\n   - Flyway (Multi-language)\n   - Liquibase (Multi-database)\n\n5. **Large Table Migrations:**\n   - Create new table structure\n   - Copy data in batches\n   - Create indexes\n   - Swap table names\n   - Drop old table\n\n```sql\n-- Safe migration example\nALTER TABLE users ADD COLUMN email_verified BOOLEAN DEFAULT FALSE;\nUPDATE users SET email_verified = TRUE WHERE verified_at IS NOT NULL;\nALTER TABLE users MODIFY COLUMN email_verified BOOLEAN NOT NULL;\n```", "token_count": 378, "meta": {"sources": [{"document": "migration_safety.pdf", "score": 0.97}]}},
                {"role": MessageRole.USER, "content": "What about connection pooling?"},
                {"role": MessageRole.ASSISTANT, "content": "Connection pooling is critical for scalability:\n\n**Why Connection Pooling?**\n- Creating DB connections is expensive\n- Pooling reuses connections\n- Reduces latency and resource usage\n- Improves throughput\n\n**Popular Pooling Libraries:**\n- HikariCP (Java)\n- SQLAlchemy (Python)\n- node-postgres (Node.js)\n- Pgbouncer (PostgreSQL proxy)\n\n**Configuration Example:**\n```python\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\n    'postgresql://user:password@localhost/db',\n    pool_size=10,           # Connections to keep in pool\n    max_overflow=20,        # Extra connections allowed\n    pool_recycle=3600,      # Recycle connections every hour\n    pool_pre_ping=True      # Test connection before using\n)\n```\n\n**Key Parameters:**\n- **Pool Size**: Number of persistent connections (5-20 typical)\n- **Max Overflow**: Additional temp connections (2-5x pool_size)\n- **Pool Timeout**: Wait time for available connection\n- **Pool Recycle**: Recycle stale connections\n\n**Best Practices:**\n- Monitor pool usage with metrics\n- Set appropriate timeout values\n- Use connection health checks\n- Scale pool size with load", "token_count": 312, "meta": {"sources": [{"document": "connection_pooling.pdf", "score": 0.95}]}},
                {"role": MessageRole.USER, "content": "How do I monitor database performance?"},
                {"role": MessageRole.ASSISTANT, "content": "Database monitoring and observability:\n\n**Key Metrics to Track:**\n1. **Query Performance**\n   - Query execution time (p50, p95, p99)\n   - Slow query log\n   - Query count and frequency\n\n2. **Resource Utilization**\n   - CPU usage\n   - Memory usage\n   - Disk I/O\n   - Network throughput\n\n3. **Connection Metrics**\n   - Active connections\n   - Connection pool utilization\n   - Connection wait time\n\n4. **Transaction Metrics**\n   - Transaction count\n   - Rollback rate\n   - Lock wait time\n\n**Monitoring Tools:**\n```\nPostgreSQL: pg_stat_statements, Pgbadger\nMySQL: Performance Schema, Percona Monitoring\nGeneral: Datadog, New Relic, CloudWatch\n```\n\n**Enable Slow Query Log (MySQL):**\n```sql\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 1;\nSET GLOBAL log_queries_not_using_indexes = 'ON';\n```\n\n**Query Analysis (PostgreSQL):**\n```sql\nSELECT query, calls, mean_time\nFROM pg_stat_statements\nORDER BY mean_time DESC\nLIMIT 10;\n```", "token_count": 356, "meta": {"sources": [{"document": "db_monitoring.pdf", "score": 0.94}]}},
            ]
        },
        {
            "title": "Database Design and Optimization",
            "messages": [
                {"role": MessageRole.USER, "content": "What are the best practices for database schema design?"},
                {"role": MessageRole.ASSISTANT, "content": "Key database design principles:\n\n1. **Normalization**: Reduce redundancy and improve data integrity\n   - 1NF: Atomic values only\n   - 2NF: Remove partial dependencies\n   - 3NF: Remove transitive dependencies\n   - BCNF: Advanced normalization\n\n2. **Entity-Relationship Model**: Design clear relationships\n   - One-to-One: Single related records\n   - One-to-Many: Single parent, multiple children\n   - Many-to-Many: Multiple parents, multiple children\n\n3. **Constraints**: Ensure data integrity\n   - PRIMARY KEY: Unique identifier\n   - FOREIGN KEY: Reference integrity\n   - UNIQUE: No duplicate values\n   - CHECK: Validate value ranges\n\n4. **Indexing**: Improve query performance\n   - Single column indexes for WHERE clauses\n   - Composite indexes for multi-column queries\n   - Avoid over-indexing (slows inserts/updates)", "token_count": 356, "meta": {"sources": [{"document": "database_design.pdf", "score": 0.98}]}},
                {"role": MessageRole.USER, "content": "How do I optimize slow SQL queries?"},
                {"role": MessageRole.ASSISTANT, "content": "Query optimization techniques:\n\n1. **Analysis Tools**:\n   - EXPLAIN PLAN: Understand query execution\n   - EXPLAIN ANALYZE: Get actual execution stats\n   - Index usage analysis\n\n2. **Common Optimizations**:\n   ```sql\n   -- Use EXPLAIN ANALYZE to identify bottlenecks\n   EXPLAIN ANALYZE SELECT * FROM users u \n   JOIN orders o ON u.id = o.user_id\n   WHERE u.created_at > '2025-01-01';\n   ```\n\n3. **Key Strategies**:\n   - Add indexes on WHERE, JOIN, and ORDER BY columns\n   - Use SELECT with specific columns, not SELECT *\n   - Use JOINS instead of subqueries when possible\n   - Limit results with appropriate WHERE clauses\n   - Use EXPLAIN to verify index usage\n   - Consider materialized views for complex queries\n   - Archive old data to reduce table size", "token_count": 289, "meta": {"sources": [{"document": "query_optimization.pdf", "score": 0.94}]}},
                {"role": MessageRole.USER, "content": "What about denormalization? When should I use it?"},
                {"role": MessageRole.ASSISTANT, "content": "Denormalization is a trade-off strategy:\n\n**When to Denormalize:**\n- High read volume, low write volume\n- Reporting/analytics queries that need aggregated data\n- Performance bottlenecks identified in normalized design\n- Complex joins affecting response time significantly\n\n**Common Denormalization Techniques:**\n\n1. **Redundant Storage**:\n   ```sql\n   -- Store user_name in orders instead of joining users table\n   CREATE TABLE orders (\n       id INT PRIMARY KEY,\n       user_id INT,\n       user_name VARCHAR(255),  -- Denormalized\n       total_amount DECIMAL(10,2),\n       created_at TIMESTAMP\n   );\n   ```\n\n2. **Aggregate Storage**:\n   ```sql\n   -- Store order_count in users table\n   CREATE TABLE users (\n       id INT PRIMARY KEY,\n       name VARCHAR(255),\n       order_count INT,  -- Updated via trigger\n       total_spent DECIMAL(10,2)  -- Updated via trigger\n   );\n   ```\n\n3. **Materialized Views**:\n   ```sql\n   -- Pre-compute complex aggregations\n   CREATE MATERIALIZED VIEW monthly_sales AS\n   SELECT DATE_TRUNC('month', created_at) as month,\n          SUM(total_amount) as total\n   FROM orders\n   GROUP BY DATE_TRUNC('month', created_at);\n   ```\n\n**Challenges:**\n- Data consistency (use triggers to update redundant data)\n- Increased storage requirements\n- More complex update logic", "token_count": 401, "meta": {"sources": [{"document": "denormalization_guide.pdf", "score": 0.92}]}},
                {"role": MessageRole.USER, "content": "Can you explain transaction isolation levels?"},
                {"role": MessageRole.ASSISTANT, "content": "Transaction isolation levels prevent concurrent access issues:\n\n**Four Isolation Levels:**\n\n1. **Read Uncommitted** (Lowest)\n   - Dirty reads possible\n   - Fastest performance\n   - Rarely used in production\n\n2. **Read Committed** (Default for most DBs)\n   - Prevents dirty reads\n   - Non-repeatable reads possible\n   - Good balance\n\n3. **Repeatable Read** (MySQL InnoDB default)\n   - Prevents dirty and non-repeatable reads\n   - Phantom reads possible\n   - Higher consistency\n\n4. **Serializable** (Highest)\n   - Prevents all anomalies\n   - Slowest performance\n   - Complete isolation\n\n**Example:**\n```sql\nSET TRANSACTION ISOLATION LEVEL REPEATABLE READ;\nBEGIN;\nSELECT balance FROM accounts WHERE id = 1;\n-- Other operations\nCOMMIT;\n```\n\n**Trade-offs:**\n- Higher isolation = more consistency but slower\n- Lower isolation = faster but data anomalies possible", "token_count": 334, "meta": {"sources": [{"document": "transaction_isolation.pdf", "score": 0.96}]}},
                {"role": MessageRole.USER, "content": "How do I handle database migrations safely?"},
                {"role": MessageRole.ASSISTANT, "content": "Safe database migration strategies:\n\n**Best Practices:**\n\n1. **Backward Compatibility**:\n   - Add columns as nullable first\n   - Don't remove columns immediately\n   - Support both old and new column names during transition\n\n2. **Deployment Steps**:\n   ```\n   1. Deploy code that can handle both old and new schema\n   2. Run migration scripts\n   3. Verify data integrity\n   4. Deploy final code using new schema\n   5. Clean up old columns/tables after verification\n   ```\n\n3. **Testing**:\n   - Test migrations on copy of production data\n   - Verify rollback procedures\n   - Test with expected data volume\n   - Check index performance after migration\n\n4. **Tools:**\n   - Alembic (Python)\n   - Flyway (Multi-language)\n   - Liquibase (Multi-database)\n\n5. **Large Table Migrations:**\n   - Create new table structure\n   - Copy data in batches\n   - Create indexes\n   - Swap table names\n   - Drop old table\n\n```sql\n-- Safe migration example\nALTER TABLE users ADD COLUMN email_verified BOOLEAN DEFAULT FALSE;\nUPDATE users SET email_verified = TRUE WHERE verified_at IS NOT NULL;\nALTER TABLE users MODIFY COLUMN email_verified BOOLEAN NOT NULL;\n```", "token_count": 378, "meta": {"sources": [{"document": "migration_safety.pdf", "score": 0.97}]}},
                {"role": MessageRole.USER, "content": "What about connection pooling?"},
                {"role": MessageRole.ASSISTANT, "content": "Connection pooling is critical for scalability:\n\n**Why Connection Pooling?**\n- Creating DB connections is expensive\n- Pooling reuses connections\n- Reduces latency and resource usage\n- Improves throughput\n\n**Popular Pooling Libraries:**\n- HikariCP (Java)\n- SQLAlchemy (Python)\n- node-postgres (Node.js)\n- Pgbouncer (PostgreSQL proxy)\n\n**Configuration Example:**\n```python\nfrom sqlalchemy import create_engine\n\nengine = create_engine(\n    'postgresql://user:password@localhost/db',\n    pool_size=10,           # Connections to keep in pool\n    max_overflow=20,        # Extra connections allowed\n    pool_recycle=3600,      # Recycle connections every hour\n    pool_pre_ping=True      # Test connection before using\n)\n```\n\n**Key Parameters:**\n- **Pool Size**: Number of persistent connections (5-20 typical)\n- **Max Overflow**: Additional temp connections (2-5x pool_size)\n- **Pool Timeout**: Wait time for available connection\n- **Pool Recycle**: Recycle stale connections\n\n**Best Practices:**\n- Monitor pool usage with metrics\n- Set appropriate timeout values\n- Use connection health checks\n- Scale pool size with load", "token_count": 312, "meta": {"sources": [{"document": "connection_pooling.pdf", "score": 0.95}]}},
                {"role": MessageRole.USER, "content": "How do I monitor database performance?"},
                {"role": MessageRole.ASSISTANT, "content": "Database monitoring and observability:\n\n**Key Metrics to Track:**\n1. **Query Performance**\n   - Query execution time (p50, p95, p99)\n   - Slow query log\n   - Query count and frequency\n\n2. **Resource Utilization**\n   - CPU usage\n   - Memory usage\n   - Disk I/O\n   - Network throughput\n\n3. **Connection Metrics**\n   - Active connections\n   - Connection pool utilization\n   - Connection wait time\n\n4. **Transaction Metrics**\n   - Transaction count\n   - Rollback rate\n   - Lock wait time\n\n**Monitoring Tools:**\n```\nPostgreSQL: pg_stat_statements, Pgbadger\nMySQL: Performance Schema, Percona Monitoring\nGeneral: Datadog, New Relic, CloudWatch\n```\n\n**Enable Slow Query Log (MySQL):**\n```sql\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 1;\nSET GLOBAL log_queries_not_using_indexes = 'ON';\n```\n\n**Query Analysis (PostgreSQL):**\n```sql\nSELECT query, calls, mean_time\nFROM pg_stat_statements\nORDER BY mean_time DESC\nLIMIT 10;\n```", "token_count": 356, "meta": {"sources": [{"document": "db_monitoring.pdf", "score": 0.94}]}},
                {"role": MessageRole.USER, "content": "What about backup and disaster recovery?"},
                {"role": MessageRole.ASSISTANT, "content": "Backup and disaster recovery strategy:\n\n**Backup Types:**\n\n1. **Full Backup**: Complete database copy\n   - Daily or weekly\n   - Large size, slower\n\n2. **Incremental Backup**: Only changes since last backup\n   - Daily increments\n   - Smaller size, faster\n\n3. **Differential Backup**: Changes since last full backup\n   - Daily differentials\n   - Medium size\n\n**Backup Strategy (3-2-1 Rule):**\n- 3 copies of data\n- 2 different media types\n- 1 offsite copy\n\n**RTO/RPO Targets:**\n- **RTO** (Recovery Time Objective): Max downtime (e.g., 1 hour)\n- **RPO** (Recovery Point Objective): Max data loss (e.g., 15 minutes)\n\n**Implementation:**\n```bash\n# PostgreSQL backup\npg_dump -U user -h host database > backup.sql\npg_dump -Fc -U user -h host database > backup.dump\n\n# MySQL backup\nmysqldump -u user -p database > backup.sql\nmysqldump --all-databases > full_backup.sql\n\n# Restore\npsql -U user -h host database < backup.sql\nmysql -u user -p database < backup.sql\n```\n\n**Cloud Solutions:**\n- AWS: RDS automated backups, automated failover\n- Google Cloud: Cloud SQL automated backups\n- Azure: SQL Database backup and restore\n\n**Testing:**\n- Regular backup restoration tests\n- Document recovery procedures\n- Estimate recovery time", "token_count": 389, "meta": {"sources": [{"document": "backup_disaster_recovery.pdf", "score": 0.96}]}},
                {"role": MessageRole.USER, "content": "Thanks for all this detailed information!"},
                {"role": MessageRole.ASSISTANT, "content": "You're very welcome! You've covered crucial database topics:\n\n✓ Schema design and normalization\n✓ Query optimization\n✓ Transaction management\n✓ Safe migrations\n✓ Connection pooling\n✓ Performance monitoring\n✓ Backup and recovery\n\nKey Takeaways:\n1. Design for scalability from the start\n2. Monitor continuously\n3. Test migrations thoroughly\n4. Plan for disaster recovery\n5. Use appropriate tools for your database\n\nGood luck with your database projects!", "token_count": 245},
            ]
        },
        {
            "title": "Docker and Containerization",
            "messages": [
                {"role": MessageRole.USER, "content": "What is Docker and why should I use it?"},
                {"role": MessageRole.ASSISTANT, "content": "Docker is a containerization platform that packages your application with all dependencies:\n\n**Benefits:**\n1. **Consistency**: Works the same on dev, test, and production\n2. **Isolation**: Each container runs independently\n3. **Scalability**: Easy to replicate containers\n4. **Portability**: Run anywhere Docker is installed\n5. **Lightweight**: Uses less resources than VMs\n\n**Core Concepts:**\n- **Image**: Blueprint for containers (immutable)\n- **Container**: Running instance of an image\n- **Registry**: Storage for images (Docker Hub, ECR, etc.)\n- **Dockerfile**: Instructions to build an image\n\n**Basic Dockerfile Example**:\n```dockerfile\nFROM python:3.11-slim\nWORKDIR /app\nCOPY requirements.txt .\nRUN pip install -r requirements.txt\nCOPY . .\nEXPOSE 8000\nCMD [\"python\", \"app.py\"]\n```", "token_count": 267, "meta": {"sources": [{"document": "docker_intro.pdf", "score": 0.97}]}},
                {"role": MessageRole.USER, "content": "How do I build and run a Docker image?"},
                {"role": MessageRole.ASSISTANT, "content": "Docker workflow:\n\n**Building an Image**:\n```bash\n# Build from Dockerfile\ndocker build -t my-app:1.0 .\n\n# Build with build arguments\ndocker build --build-arg PYTHON_VERSION=3.11 -t my-app:1.0 .\n```\n\n**Running a Container**:\n```bash\n# Basic run\ndocker run my-app:1.0\n\n# With port mapping\ndocker run -p 8000:8000 my-app:1.0\n\n# With environment variables\ndocker run -e DATABASE_URL=postgresql://localhost/mydb -p 8000:8000 my-app:1.0\n\n# Interactive mode (bash)\ndocker run -it my-app:1.0 /bin/bash\n\n# Background (detached)\ndocker run -d --name myapp -p 8000:8000 my-app:1.0\n```\n\n**Common Commands**:\n```bash\n# List images\ndocker images\n\n# List running containers\ndocker ps\n\n# List all containers (including stopped)\ndocker ps -a\n\n# Stop container\ndocker stop myapp\n\n# Remove image\ndocker rmi my-app:1.0\n```", "token_count": 298, "meta": {"sources": [{"document": "docker_commands.pdf", "score": 0.96}]}},
                {"role": MessageRole.USER, "content": "What about Docker Compose for multi-container applications?"},
                {"role": MessageRole.ASSISTANT, "content": "Docker Compose simplifies multi-container orchestration:\n\n**docker-compose.yml Example**:\n```yaml\nversion: '3.8'\n\nservices:\n  web:\n    build: .\n    ports:\n      - \"8000:8000\"\n    environment:\n      DATABASE_URL: postgresql://db:5432/myapp\n    depends_on:\n      - db\n    volumes:\n      - ./app:/app\n\n  db:\n    image: postgres:15\n    environment:\n      POSTGRES_DB: myapp\n      POSTGRES_PASSWORD: secret\n    volumes:\n      - postgres_data:/var/lib/postgresql/data\n\n  redis:\n    image: redis:7-alpine\n    ports:\n      - \"6379:6379\"\n\nvolumes:\n  postgres_data:\n```\n\n**Common Commands**:\n```bash\n# Start all services\ndocker-compose up\n\n# Start in background\ndocker-compose up -d\n\n# Stop services\ndocker-compose down\n\n# View logs\ndocker-compose logs -f web\n\n# Execute command in container\ndocker-compose exec web python manage.py migrate\n\n# Rebuild images\ndocker-compose build\n```\n\nDocker Compose is essential for local development with multiple interdependent services!", "token_count": 334, "meta": {"sources": [{"document": "docker_compose_guide.pdf", "score": 0.95}]}},
                {"role": MessageRole.USER, "content": "How do I optimize Docker images?"},
                {"role": MessageRole.ASSISTANT, "content": "Docker image optimization techniques:\n\n**1. Use Smaller Base Images:**\n```dockerfile\n# Bad - large image\nFROM ubuntu:22.04\n\n# Good - minimal\nFROM python:3.11-slim\n\n# Better - ultra-minimal\nFROM python:3.11-alpine\n```\n\n**2. Minimize Layers:**\n```dockerfile\n# Bad - multiple layers\nRUN apt-get update\nRUN apt-get install -y curl\nRUN apt-get clean\n\n# Good - single layer\nRUN apt-get update && apt-get install -y curl && apt-get clean\n```\n\n**3. Use .dockerignore:**\n```\n.git\n.gitignore\n__pycache__\n*.pyc\n.pytest_cache\n.env\nnode_modules\n```\n\n**4. Multi-stage Builds:**\n```dockerfile\n# Build stage\nFROM node:18 AS builder\nWORKDIR /app\nCOPY package*.json .\nRUN npm ci\nCOPY . .\nRUN npm run build\n\n# Runtime stage\nFROM node:18-alpine\nWORKDIR /app\nCOPY --from=builder /app/dist ./dist\nCOPY --from=builder /app/node_modules ./node_modules\nCMD [\"node\", \"dist/index.js\"]\n```\n\n**5. Cache Optimization:**\n- Order Dockerfile commands from least to most frequently changed\n- Dependencies before code\n- Use layer caching effectively", "token_count": 345, "meta": {"sources": [{"document": "docker_optimization.pdf", "score": 0.94}]}},
                {"role": MessageRole.USER, "content": "What about Docker networking?"},
                {"role": MessageRole.ASSISTANT, "content": "Docker networking overview:\n\n**Network Types:**\n\n1. **Bridge Network** (default)\n   - Isolated network for containers\n   - Containers communicate via IP or name\n   ```bash\n   docker network create my-network\n   docker run --network my-network --name web myapp\n   docker run --network my-network -e DATABASE_HOST=web postgres\n   ```\n\n2. **Host Network**\n   - Uses host's network directly\n   - Better performance, less isolation\n   ```bash\n   docker run --network host myapp\n   ```\n\n3. **None Network**\n   - No networking\n   ```bash\n   docker run --network none myapp\n   ```\n\n**Port Mapping:**\n```bash\n# Map single port\ndocker run -p 8000:8000 myapp\n\n# Map multiple ports\ndocker run -p 8000:8000 -p 3000:3000 myapp\n\n# Map to random port\ndocker run -p 8000 myapp\n```\n\n**Docker Compose Networking:**\n```yaml\nservices:\n  web:\n    image: myapp\n    # Automatically joins compose network\n    # Can reach 'db' via hostname\n  db:\n    image: postgres\n    # 'web' service can reach this via 'db' hostname\n```\n\nContainers in same Compose project can reach each other by service name!", "token_count": 312, "meta": {"sources": [{"document": "docker_networking.pdf", "score": 0.95}]}},
                {"role": MessageRole.USER, "content": "How do I handle persistent data in Docker?"},
                {"role": MessageRole.ASSISTANT, "content": "Managing data in Docker containers:\n\n**1. Volumes (Recommended)**\n   - Managed by Docker\n   - Persist after container deletion\n   - Can be shared between containers\n\n```bash\n# Create volume\ndocker volume create mydata\n\n# Use volume\ndocker run -v mydata:/data myapp\n\n# Named volume in Compose\nversion: '3.8'\nservices:\n  db:\n    image: postgres\n    volumes:\n      - db_data:/var/lib/postgresql/data\nvolumes:\n  db_data:\n```\n\n**2. Bind Mounts**\n   - Map host directory to container\n   - Useful for development\n   ```bash\n   docker run -v /host/path:/container/path myapp\n   ```\n\n**3. tmpfs Mounts**\n   - Temporary, in-memory storage\n   - Lost when container stops\n   ```bash\n   docker run --tmpfs /app/tmp myapp\n   ```\n\n**Volume Management:**\n```bash\n# List volumes\ndocker volume ls\n\n# Inspect volume\ndocker volume inspect mydata\n\n# Remove volume\ndocker volume rm mydata\n\n# Backup volume\ndocker run --rm -v mydata:/data -v $(pwd):/backup ubuntu tar cvf /backup/backup.tar /data\n```\n\n**Best Practices:**\n- Use volumes for databases\n- Use bind mounts for development\n- Document volume requirements\n- Regular backups of important volumes", "token_count": 367, "meta": {"sources": [{"document": "docker_volumes.pdf", "score": 0.93}]}},
                {"role": MessageRole.USER, "content": "What are Docker security best practices?"},
                {"role": MessageRole.ASSISTANT, "content": "Docker security essentials:\n\n**1. Use Official Images:**\n```dockerfile\n# Good\nFROM python:3.11-slim\n\n# Bad\nFROM random-python-image:latest\n```\n\n**2. Scan for Vulnerabilities:**\n```bash\n# Docker Scout\ndocker scout cves myapp:latest\n\n# Trivy\ntrivy image myapp:latest\n```\n\n**3. Run as Non-Root User:**\n```dockerfile\nFROM python:3.11-slim\nRUN useradd -m appuser\nUSER appuser\nCMD [\"python\", \"app.py\"]\n```\n\n**4. Use Read-Only Filesystem:**\n```bash\ndocker run --read-only -v /app/tmp:/app/tmp myapp\n```\n\n**5. Limit Resources:**\n```bash\n# Memory limit\ndocker run -m 512m myapp\n\n# CPU limit\ndocker run --cpus=\"1.5\" myapp\n\n# Compose example\nservices:\n  web:\n    image: myapp\n    deploy:\n      resources:\n        limits:\n          cpus: '1.0'\n          memory: 512M\n```\n\n**6. Secrets Management:**\n```bash\n# Never put secrets in images\n# Use environment variables\ndocker run -e DATABASE_PASSWORD=secret myapp\n\n# Or use Docker secrets (Swarm)\ndocker secret create db_password -\ndocker service create --secret db_password myapp\n```\n\n**7. Keep Images Updated:**\n- Regularly rebuild images\n- Update base images\n- Scan dependencies for CVEs", "token_count": 378, "meta": {"sources": [{"document": "docker_security.pdf", "score": 0.96}]}},
                {"role": MessageRole.USER, "content": "How do I debug containerized applications?"},
                {"role": MessageRole.ASSISTANT, "content": "Debugging Docker containers:\n\n**1. View Logs:**\n```bash\n# Container logs\ndocker logs myapp\n\n# Follow logs in real-time\ndocker logs -f myapp\n\n# Show last 100 lines\ndocker logs --tail 100 myapp\n\n# Compose logs\ndocker-compose logs -f web\n```\n\n**2. Execute Commands in Container:**\n```bash\n# Interactive shell\ndocker exec -it myapp /bin/bash\n\n# Run command\ndocker exec myapp python -c \"print('test')\"\n\n# With elevated privileges\ndocker exec -u root myapp apt-get update\n```\n\n**3. Inspect Container:**\n```bash\n# Container details\ndocker inspect myapp\n\n# Format output\ndocker inspect --format='{{.State.Status}}' myapp\n```\n\n**4. Monitor Resources:**\n```bash\n# Real-time stats\ndocker stats\n\n# Stats for specific container\ndocker stats myapp\n```\n\n**5. Debug Using Debug Container:**\n```bash\n# Use debug image\ndocker run -it --rm -v /var/run/docker.sock:/var/run/docker.sock \\\n  --network container:myapp \\\n  nicolaka/netshoot\n\n# Inside container: use tools like curl, netstat, tcpdump\n```\n\n**6. View Container Processes:**\n```bash\n# Process list\ndocker top myapp\n```\n\n**Common Issues:**\n- Container exits: Check logs for errors\n- Network issues: Verify container networking\n- Resource issues: Check docker stats output", "token_count": 334, "meta": {"sources": [{"document": "docker_debugging.pdf", "score": 0.94}]}},
                {"role": MessageRole.USER, "content": "Great comprehensive overview! Thanks!"},
                {"role": MessageRole.ASSISTANT, "content": "You're welcome! Key Docker takeaways:\n\n✓ Containerize applications for consistency\n✓ Use Docker Compose for local development\n✓ Optimize images for size and security\n✓ Implement proper networking and storage\n✓ Follow security best practices\n✓ Use logs and tools for debugging\n✓ Monitor container resources\n✓ Plan disaster recovery\n\nGood luck with your containerization journey!", "token_count": 201},
            ]
        },
        {
            "title": "API Design and REST Principles",
            "messages": [
                {"role": MessageRole.USER, "content": "What makes a good REST API design?"},
                {"role": MessageRole.ASSISTANT, "content": "RESTful API design principles:\n\n**Core Concepts**:\n1. **Resources**: Use nouns (users, posts, comments)\n2. **HTTP Methods**: GET, POST, PUT, DELETE, PATCH\n3. **Status Codes**: 200 OK, 201 Created, 400 Bad Request, 404 Not Found, 500 Server Error\n4. **Versioning**: /api/v1/, /api/v2/\n5. **Pagination**: Limit, offset, cursor-based\n6. **Filtering**: Query parameters for filtering\n7. **Sorting**: Field and direction parameters\n\n**Endpoint Examples**:\n```\nGET    /api/v1/users                 # List users\nPOST   /api/v1/users                 # Create user\nGET    /api/v1/users/{id}            # Get user\nPUT    /api/v1/users/{id}            # Replace user\nPATCH  /api/v1/users/{id}            # Update user\nDELETE /api/v1/users/{id}            # Delete user\nGET    /api/v1/users?role=admin      # Filter\nGET    /api/v1/users?sort=created_at # Sort\nGET    /api/v1/users?limit=10&offset=20  # Pagination\n```\n\n**Response Format**:\n```json\n{\n  \"status\": \"success\",\n  \"data\": {\n    \"id\": 1,\n    \"name\": \"John Doe\",\n    \"email\": \"john@example.com\"\n  },\n  \"meta\": {\n    \"timestamp\": \"2025-11-23T10:30:00Z\",\n    \"version\": \"1.0\"\n  }\n}\n```", "token_count": 378, "meta": {"sources": [{"document": "rest_api_design.pdf", "score": 0.98}]}},
                {"role": MessageRole.USER, "content": "How should I handle errors in APIs?"},
                {"role": MessageRole.ASSISTANT, "content": "Error handling best practices:\n\n**Consistent Error Response Format**:\n```json\n{\n  \"status\": \"error\",\n  \"error\": {\n    \"code\": \"INVALID_REQUEST\",\n    \"message\": \"User not found\",\n    \"details\": {\n      \"field\": \"user_id\",\n      \"value\": \"999\"\n    }\n  },\n  \"meta\": {\n    \"timestamp\": \"2025-11-23T10:30:00Z\",\n    \"request_id\": \"req_12345\"\n  }\n}\n```\n\n**HTTP Status Codes**:\n- 400 Bad Request: Invalid input validation\n- 401 Unauthorized: Authentication required\n- 403 Forbidden: Insufficient permissions\n- 404 Not Found: Resource doesn't exist\n- 409 Conflict: Duplicate resource or state conflict\n- 422 Unprocessable Entity: Semantic validation error\n- 500 Internal Server Error: Unexpected server error\n- 503 Service Unavailable: Maintenance/overload\n\n**Error Code Standards**:\n```\nINVALID_REQUEST: Validation failed\nUNAUTHENTICATED: Login required\nUNAUTHORIZED: Permission denied\nNOT_FOUND: Resource doesn't exist\nCONFLICT: Duplicate or state error\nINTERNAL_ERROR: Unexpected server error\nSERVICE_UNAVAILABLE: Temporary issue\n```\n\n**Implementation**:\n```python\n@app.exception_handler(ValueError)\nasync def value_error_handler(request, exc):\n    return JSONResponse(\n        status_code=400,\n        content={\n            \"status\": \"error\",\n            \"error\": {\n                \"code\": \"INVALID_REQUEST\",\n                \"message\": str(exc)\n            }\n        }\n    )\n```", "token_count": 389, "meta": {"sources": [{"document": "api_error_handling.pdf", "score": 0.96}]}},
                {"role": MessageRole.USER, "content": "What about authentication and authorization?"},
                {"role": MessageRole.ASSISTANT, "content": "Security in APIs:\n\n**Authentication Methods**:\n1. **API Keys**: Simple, good for internal APIs\n   ```bash\n   curl -H \"X-API-Key: sk_live_abc123\" https://api.example.com/users\n   ```\n\n2. **OAuth 2.0**: Industry standard for third-party access\n   ```bash\n   curl -H \"Authorization: Bearer token_xyz\" https://api.example.com/users\n   ```\n\n3. **JWT (JSON Web Tokens)**: Stateless authentication\n   ```python\n   from fastapi import Depends\n   from fastapi.security import HTTPBearer\n   \n   security = HTTPBearer()\n   \n   async def get_current_user(credentials: HTTPAuthCredentials = Depends(security)):\n       token = credentials.credentials\n       # Verify and decode JWT\n   ```\n\n**Authorization (Role-Based Access Control)**:\n```python\nfrom fastapi import HTTPException\n\nasync def require_admin(current_user = Depends(get_current_user)):\n    if \"admin\" not in current_user.roles:\n        raise HTTPException(status_code=403, detail=\"Admin access required\")\n    return current_user\n\n@app.delete(\"/api/v1/users/{id}\")\nasync def delete_user(id: int, admin = Depends(require_admin)):\n    # Only admins can delete users\n    pass\n```\n\n**Security Best Practices**:\n- ✓ Use HTTPS/TLS for all API traffic\n- ✓ Implement rate limiting to prevent abuse\n- ✓ Validate and sanitize all inputs\n- ✓ Use strong secret keys and rotate regularly\n- ✓ Log security events\n- ✓ Implement request signing for sensitive operations\n- ✓ Use short token expiration times\n- ✓ Refresh tokens separately from access tokens", "token_count": 425, "meta": {"sources": [{"document": "api_security.pdf", "score": 0.97}]}},
                {"role": MessageRole.USER, "content": "How do I implement API rate limiting?"},
                {"role": MessageRole.ASSISTANT, "content": "API rate limiting strategies:\n\n**1. Token Bucket Algorithm**:\n   - Tokens refill at steady rate\n   - Allows burst traffic\n   - Most common approach\n\n**2. Sliding Window**:\n   - Track requests in time window\n   - More accurate but memory intensive\n\n**3. Fixed Window**:\n   - Simple, less accurate\n   - Resets at fixed intervals\n\n**Implementation Examples**:\n\n```python\n# Using slowapi\nfrom slowapi import Limiter\nfrom slowapi.util import get_remote_address\n\nlimiter = Limiter(key_func=get_remote_address)\napp = FastAPI()\napp.state.limiter = limiter\n\n@app.get(\"/api/v1/users\")\n@limiter.limit(\"100/minute\")\nasync def list_users(request: Request):\n    return {\"users\": []}\n```\n\n```python\n# Using Redis\nfrom aioredis import Redis\n\nasync def check_rate_limit(user_id: str, redis: Redis, limit: int = 100, window: int = 60):\n    key = f\"rate_limit:{user_id}\"\n    current = await redis.incr(key)\n    if current == 1:\n        await redis.expire(key, window)\n    if current > limit:\n        raise HTTPException(status_code=429, detail=\"Rate limit exceeded\")\n    return current\n```\n\n**Response Headers**:\n```\nX-RateLimit-Limit: 100\nX-RateLimit-Remaining: 95\nX-RateLimit-Reset: 1700736000\nRetry-After: 60\n```\n\n**Best Practices**:\n- Different limits for different endpoints\n- User-based vs IP-based limits\n- Generous limits for authenticated users\n- Return 429 (Too Many Requests) status code", "token_count": 367, "meta": {"sources": [{"document": "rate_limiting.pdf", "score": 0.95}]}},
                {"role": MessageRole.USER, "content": "What about API documentation and versioning?"},
                {"role": MessageRole.ASSISTANT, "content": "API Documentation and Versioning:\n\n**1. Documentation Tools**:\n   - **Swagger/OpenAPI**: Interactive API docs\n   - **ReDoc**: Beautiful documentation\n   - **AsyncAPI**: For event-driven APIs\n\n**2. FastAPI Example**:\n```python\nfrom fastapi import FastAPI\nfrom pydantic import BaseModel\n\napp = FastAPI(\n    title=\"My API\",\n    description=\"API for managing users\",\n    version=\"1.0.0\",\n    docs_url=\"/api/docs\",\n    redoc_url=\"/api/redoc\"\n)\n\nclass User(BaseModel):\n    \"\"\"User model\"\"\"\n    id: int\n    name: str\n    email: str\n\n@app.get(\"/api/v1/users/{user_id}\", response_model=User)\nasync def get_user(user_id: int):\n    \"\"\"Get user by ID\"\"\"\n    pass\n```\n\n**3. Versioning Strategies**:\n\n**URL Versioning** (Most common):\n```\nGET /api/v1/users\nGET /api/v2/users\n```\n\n**Header Versioning**:\n```\nGET /api/users\nHeader: Accept-Version: 1.0\n```\n\n**Query Parameter Versioning**:\n```\nGET /api/users?version=1\n```\n\n**4. Migration Strategy**:\n```python\n@app.get(\"/api/v1/users\")\nasync def get_users_v1():\n    # Old version - deprecated\n    pass\n\n@app.get(\"/api/v2/users\")\nasync def get_users_v2():\n    # New version with improvements\n    pass\n```\n\n**5. Deprecation Policy**:\n- Announce deprecation in headers\n- Provide 6+ month migration period\n- Clear documentation\n- Support multiple versions during transition", "token_count": 389, "meta": {"sources": [{"document": "api_documentation.pdf", "score": 0.94}]}},
                {"role": MessageRole.USER, "content": "How do I optimize API performance?"},
                {"role": MessageRole.ASSISTANT, "content": "API Performance Optimization:\n\n**1. Caching Strategies**:\n```python\nfrom fastapi import Response\nfrom datetime import datetime, timedelta\n\n@app.get(\"/api/v1/users/{id}\")\nasync def get_user(id: int, response: Response):\n    user = await fetch_user(id)\n    \n    # Cache for 5 minutes\n    response.headers[\"Cache-Control\"] = \"public, max-age=300\"\n    response.headers[\"ETag\"] = hash(user)\n    \n    return user\n```\n\n**2. Database Query Optimization**:\n```python\n# Use query pagination\n@app.get(\"/api/v1/users\")\nasync def list_users(limit: int = 10, offset: int = 0):\n    users = await db.query(User).limit(limit).offset(offset).all()\n    return users\n\n# Use eager loading\nstmt = select(User).options(selectinload(User.posts))\nusers = await session.execute(stmt)\n```\n\n**3. Response Compression**:\n```python\nfrom fastapi.middleware.gzip import GZIPMiddleware\n\napp.add_middleware(GZIPMiddleware, minimum_size=1000)\n```\n\n**4. Async Operations**:\n```python\n@app.post(\"/api/v1/emails/send\")\nasync def send_email(email_data: EmailSchema, background_tasks: BackgroundTasks):\n    # Return immediately, process in background\n    background_tasks.add_task(send_email_task, email_data)\n    return {\"status\": \"queued\"}\n```\n\n**5. Connection Pooling**:\n```python\nengine = create_async_engine(\n    DATABASE_URL,\n    echo=False,\n    pool_size=20,\n    max_overflow=0,\n    pool_pre_ping=True,\n    pool_recycle=3600,\n)\n```\n\n**6. Monitoring and Metrics**:\n```python\nfrom prometheus_client import Counter, Histogram\n\nrequest_count = Counter('api_requests_total', 'Total API requests')\nresponse_time = Histogram('api_response_time_seconds', 'API response time')\n\n@app.get(\"/api/v1/users\")\n@response_time.time()\nasync def list_users():\n    request_count.inc()\n    return {\"users\": []}\n```", "token_count": 356, "meta": {"sources": [{"document": "api_performance.pdf", "score": 0.96}]}},
                {"role": MessageRole.USER, "content": "What are common API mistakes to avoid?"},
                {"role": MessageRole.ASSISTANT, "content": "Common API Design Mistakes:\n\n**1. Inconsistent Naming**:\n```python\n# Bad\n@app.get(\"/getUser\")\n@app.post(\"/users/add\")\n@app.delete(\"/deleteUser/{id}\")\n\n# Good\n@app.get(\"/users/{id}\")\n@app.post(\"/users\")\n@app.delete(\"/users/{id}\")\n```\n\n**2. Ignoring Pagination**:\n```python\n# Bad - returns all records\n@app.get(\"/users\")\nasync def list_users():\n    return db.query(User).all()  # Could be millions!\n\n# Good - with pagination\n@app.get(\"/users\")\nasync def list_users(limit: int = 10, offset: int = 0):\n    return db.query(User).limit(limit).offset(offset).all()\n```\n\n**3. Poor Error Messages**:\n```python\n# Bad\nraise HTTPException(status_code=500, detail=\"Error\")\n\n# Good\nraise HTTPException(\n    status_code=400, \n    detail=\"Invalid email format. Expected format: user@example.com\"\n)\n```\n\n**4. Not Using HTTP Methods Correctly**:\n```python\n# Bad\n@app.get(\"/users/delete?id=1\")  # GET for deletion!\n@app.post(\"/users/list\")        # POST for listing!\n\n# Good\n@app.get(\"/users\")              # List\n@app.post(\"/users\")             # Create\n@app.delete(\"/users/{id}\")      # Delete\n```\n\n**5. Mixing Data and Metadata**:\n```python\n# Bad\n{\"id\": 1, \"name\": \"John\", \"success\": true, \"timestamp\": \"2025-11-23T10:00:00Z\"}\n\n# Good\n{\n  \"status\": \"success\",\n  \"data\": {\"id\": 1, \"name\": \"John\"},\n  \"meta\": {\"timestamp\": \"2025-11-23T10:00:00Z\"}\n}\n```\n\n**6. Breaking Changes Without Versioning**:\n- Always version your API\n- Maintain backward compatibility\n- Communicate deprecation clearly\n\n**7. Security Issues**:\n- No authentication/authorization\n- Exposing sensitive data\n- SQL injection vulnerabilities\n- Missing input validation", "token_count": 402, "meta": {"sources": [{"document": "api_mistakes.pdf", "score": 0.95}]}},
                {"role": MessageRole.USER, "content": "Perfect comprehensive guide! Thank you!"},
                {"role": MessageRole.ASSISTANT, "content": "You're welcome! API Design Checklist:\n\n✓ Use RESTful principles (nouns, HTTP methods)\n✓ Version your API\n✓ Implement proper error handling\n✓ Add authentication and authorization\n✓ Rate limit to prevent abuse\n✓ Implement caching strategies\n✓ Use pagination for large datasets\n✓ Document thoroughly with OpenAPI\n✓ Monitor performance and metrics\n✓ Plan deprecation carefully\n✓ Validate and sanitize all inputs\n✓ Use appropriate HTTP status codes\n\nGood luck building great APIs!", "token_count": 234},
            ]
        },
        {
            "title": "Performance Testing and Monitoring",
            "messages": [
                {"role": MessageRole.USER, "content": "How do I identify performance bottlenecks in my application?"},
                {"role": MessageRole.ASSISTANT, "content": "Performance analysis approach:\n\n**1. Monitoring Tools**:\n- **APM (Application Performance Monitoring)**: New Relic, Datadog, Elastic\n- **Profilers**: Python cProfile, Java JProfiler\n- **Logging**: ELK Stack, Splunk\n- **Metrics**: Prometheus, Grafana\n\n**2. Key Metrics**:\n- Response Time (p50, p95, p99 percentiles)\n- Throughput (requests per second)\n- Error Rate\n- Resource Usage (CPU, Memory, Disk I/O, Network)\n- Database Query Time\n- Cache Hit Ratio\n\n**3. Python Profiling Example**:\n```python\nimport cProfile\nimport pstats\nfrom pstats import SortKey\n\n# Method 1: Using profiler\ncProfile.run('slow_function()', 'profstats')\nstats = pstats.Stats('profstats')\nstats.sort_stats(SortKey.CUMULATIVE).print_stats()\n\n# Method 2: Decorator\nimport time\ndef profile(func):\n    def wrapper(*args, **kwargs):\n        start = time.time()\n        result = func(*args, **kwargs)\n        print(f\"{func.__name__} took {time.time() - start:.2f}s\")\n        return result\n    return wrapper\n\n@profile\ndef slow_function():\n    pass\n```\n\n**4. Database Performance**:\n```sql\n-- Enable slow query log\nSET GLOBAL slow_query_log = 'ON';\nSET GLOBAL long_query_time = 1;\n\n-- Analyze query execution\nEXPLAIN ANALYZE SELECT * FROM large_table WHERE condition;\n```", "token_count": 389, "meta": {"sources": [{"document": "performance_monitoring.pdf", "score": 0.98}]}},
                {"role": MessageRole.USER, "content": "What optimization techniques should I prioritize?"},
                {"role": MessageRole.ASSISTANT, "content": "Optimization priority matrix:\n\n**High Impact, Easy to Implement**:\n1. **Caching**\n   - Browser caching (ETag, Cache-Control headers)\n   - Server-side caching (Redis, Memcached)\n   - Query result caching\n   - HTTP caching strategies\n\n2. **Database Optimization**\n   - Add indexes on frequently queried columns\n   - Query optimization (joins, subqueries)\n   - Connection pooling\n\n3. **Code-Level Optimizations**\n   - Lazy loading\n   - Batch operations\n   - Async operations\n\n**Medium Impact, Medium Effort**:\n1. **API Optimization**\n   - Pagination instead of loading all data\n   - Field selection (only request needed fields)\n   - Compression (gzip, brotli)\n\n2. **Resource Optimization**\n   - CDN for static assets\n   - Image optimization\n   - Minification of JS/CSS\n\n**Lower Priority (unless critical)**:\n1. Microservices migration\n2. Database replication\n3. Advanced caching strategies\n\n**Measurement Template**:\n```python\n# Before optimization\nimport time\nstart = time.time()\nresult = operation()\nend = time.time()\nprint(f\"Operation took {end - start:.3f}s\")\nprint(f\"Memory used: {memory_usage()}MB\")\n\n# After optimization - Compare results\n```", "token_count": 356, "meta": {"sources": [{"document": "optimization_priorities.pdf", "score": 0.94}]}},
                {"role": MessageRole.USER, "content": "Excellent guidance! I'll implement these systematically."},
                {"role": MessageRole.ASSISTANT, "content": "Great approach! Remember:\n1. Measure first (don't optimize blindly)\n2. Focus on high-impact items\n3. Use proper monitoring tools\n4. Test thoroughly after each change\n5. Keep good documentation\n6. Monitor in production continuously\n\nRegular performance reviews help catch regressions early. Good luck!", "token_count": 167},
            ]
        },
    ]
    
    async def run(self, session: AsyncSession, **kwargs) -> dict:
        """Create sample conversations and messages for the admin user."""
        try:
            # Validate required tables exist
            tables_exist, missing_tables = await self.validate_tables_exist(session)
            if not tables_exist:
                error_msg = (
                    f"Required table(s) missing: {', '.join(missing_tables)}. "
                    "Please run migrations first: python migrate.py"
                )
                logger.error(error_msg)
                return {"success": False, "message": error_msg}
            
            # Get admin user (user_id = 1)
            admin_user_id = kwargs.get("user_id", 1)
            stmt = select(User).where(User.id == admin_user_id)
            result = await session.execute(stmt)
            admin_user = result.scalar_one_or_none()
            
            if not admin_user:
                error_msg = f"User with ID {admin_user_id} not found"
                logger.error(error_msg)
                return {"success": False, "message": error_msg}
            
            created_conversations = 0
            created_messages = 0
            base_time = datetime.now(timezone.utc) - timedelta(days=30)
            
            # Create conversations
            for i, conv_data in enumerate(self.CONVERSATIONS):
                # Check if conversation already exists
                stmt = select(Conversation).where(
                    Conversation.user_id == admin_user_id,
                    Conversation.title == conv_data["title"]
                )
                result = await session.execute(stmt)
                if result.scalar_one_or_none():
                    logger.debug(f"Conversation '{conv_data['title']}' already exists")
                    continue
                
                # Create conversation
                conversation = Conversation(
                    id=str(uuid.uuid4()),
                    user_id=admin_user_id,
                    title=conv_data["title"],
                    message_count=len(conv_data["messages"]),
                    last_message_at=base_time + timedelta(days=i),
                )
                session.add(conversation)
                await session.flush()
                created_conversations += 1
                
                # Create messages
                for msg_index, msg_data in enumerate(conv_data["messages"]):
                    message = Message(
                        id=str(uuid.uuid4()),
                        conversation_id=conversation.id,
                        role=msg_data["role"],
                        content=msg_data["content"],
                        token_count=msg_data.get("token_count"),
                        meta=msg_data.get("meta"),
                    )
                    # Stagger message timestamps
                    message.created_at = base_time + timedelta(days=i, hours=msg_index)
                    message.updated_at = message.created_at
                    
                    session.add(message)
                    created_messages += 1
                
                logger.info(f"Created conversation: {conv_data['title']} with {len(conv_data['messages'])} messages")
            
            if created_conversations > 0 or created_messages > 0:
                await session.commit()
                message = f"Created {created_conversations} conversations with {created_messages} messages"
                logger.info(f"✓ {message}")
                return {"success": True, "message": message}
            
            return {"success": True, "message": "All conversations already exist"}
            
        except Exception as e:
            await session.rollback()
            logger.error(f"Failed to create conversations: {e}", exc_info=True)
            return {"success": False, "message": str(e)}
