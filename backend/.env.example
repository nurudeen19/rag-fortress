# Application Settings
APP_NAME=RAG Fortress
APP_VERSION=1.0.0
DEBUG=True
ENVIRONMENT=development
HOST=0.0.0.0
PORT=8000

# LLM Provider Selection (openai, google, huggingface)
LLM_PROVIDER=openai

# Fallback LLM Configuration (optional - defaults to small HuggingFace model if not set)
# Option 1: Use custom fallback model (recommended)
FALLBACK_LLM_PROVIDER=openai
FALLBACK_LLM_API_KEY=your_api_key_here
FALLBACK_LLM_MODEL=gpt-3.5-turbo
FALLBACK_LLM_TEMPERATURE=0.5
FALLBACK_LLM_MAX_TOKENS=1000

# Option 2: Use provider's default config (omit custom fields above)
# FALLBACK_LLM_PROVIDER=google

# Option 3: Leave empty to use default small HuggingFace model
# (google/flan-t5-small - no API key required if using inference API)

# OpenAI Configuration
OPENAI_API_KEY=your_openai_api_key_here
OPENAI_MODEL=gpt-3.5-turbo
OPENAI_TEMPERATURE=0.7
OPENAI_MAX_TOKENS=2000

# Google Configuration
GOOGLE_API_KEY=your_google_api_key_here
GOOGLE_MODEL=gemini-pro
GOOGLE_TEMPERATURE=0.7
GOOGLE_MAX_TOKENS=2000

# HuggingFace Configuration
HF_API_TOKEN=your_huggingface_token_here
HF_MODEL=meta-llama/Llama-2-7b-chat-hf
HF_TEMPERATURE=0.7
HF_MAX_TOKENS=2000


# Database Configuration
DATABASE_URL=sqlite:///./rag_fortress.db
# DATABASE_URL=postgresql://user:password@localhost:5432/rag_fortress

# Vector Database
CHROMA_PERSIST_DIRECTORY=./chroma_db
EMBEDDING_MODEL=all-MiniLM-L6-v2

# RAG Configuration
CHUNK_SIZE=1000
CHUNK_OVERLAP=200
TOP_K_RESULTS=5

# Security
SECRET_KEY=your_secret_key_here_change_in_production
ALGORITHM=HS256
ACCESS_TOKEN_EXPIRE_MINUTES=30

# CORS
ALLOWED_ORIGINS=http://localhost:3000,http://localhost:8000

# Logging
LOG_LEVEL=INFO
LOG_FILE=logs/app.log
